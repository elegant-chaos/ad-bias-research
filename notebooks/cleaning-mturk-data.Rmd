---
title: "cleaning_mturk_data"
author: "Jenn Halbleib"
date: "11/25/2019"
output: html_document
---

```{r}
library(tidyverse)
```

Clean Data

```{r}
dat <- read_csv("https://raw.githubusercontent.com/elegant-chaos/google-ad-bias-research/master/mturk/python/combined_csv_zipandcountry.csv")

ad_key_dat <- read_csv("/Users/mm06682/projects/school_projects/fall_2019/software_engineering/google-ad-bias-research/data/processed/adDetail.csv")

# How many workers completed more than one HIT?
duplicate_workers <- dat %>% group_by(workerId) %>% summarise(n = n()) %>% filter(n > 1)
write_csv(duplicate_workers,'/Users/mm06682/projects/school_projects/fall_2019/software_engineering/google-ad-bias-research/data/processed/duplicate_workers.csv')

# Remove duplicated workers by randomly selecting 1 row for each worker
dat <- dat %>% group_by(workerId) %>% mutate(n = n()) %>% sample_n(1) %>% ungroup()

# Remove rows without the country criteria set
dat <- dat %>% filter(CountryCriteria == "Yes")

# Make a long form data set
get_dat_long_for_one_image <- function(dat, q, image_file) {
  en_q <- sym(q)
  en_image_file <- sym(q)
  
  to_return <- dat %>% 
    select(c(2:16, q, image_file, 49:56)) %>% 
    rename(rating := !!q, 
           image := !!image_file) 
  
  return(to_return)
}

dat1 <- dat %>% get_dat_long_for_one_image('q1', 'image1')
dat2 <- dat %>% get_dat_long_for_one_image('q2', 'image2')
dat3 <- dat %>% get_dat_long_for_one_image('q3', 'image3')
dat4 <- dat %>% get_dat_long_for_one_image('q4', 'image4')
dat5 <- dat %>% get_dat_long_for_one_image('q5', 'image5')
dat6 <- dat %>% get_dat_long_for_one_image('q6', 'image6')
dat7 <- dat %>% get_dat_long_for_one_image('q7', 'image7')
dat8 <- dat %>% get_dat_long_for_one_image('q8', 'image8')
dat9 <- dat %>% get_dat_long_for_one_image('q9', 'image9')
dat10 <- dat %>% get_dat_long_for_one_image('q10', 'image10')
dat11 <- dat %>% get_dat_long_for_one_image('q11', 'image11')
dat12 <- dat %>% get_dat_long_for_one_image('q12', 'image12')
dat13 <- dat %>% get_dat_long_for_one_image('q13', 'image13')
dat14 <- dat %>% get_dat_long_for_one_image('q14', 'image14')
dat15 <- dat %>% get_dat_long_for_one_image('q15', 'image15')
dat16 <- dat %>% get_dat_long_for_one_image('q16', 'image16')

dat_long <- rbind(dat1, dat2, dat3, dat4, dat5, dat6, dat7, 
                  dat8, dat9, dat10, dat11, dat12, dat13, dat14, dat15, dat16)
rm(dat1, dat2, dat3, dat4, dat5, dat6, dat7,
   dat8, dat9, dat10, dat11, dat12, dat13, dat14, dat15, dat16)

dat_long <- dat_long %>% left_join(ad_key_dat, by = c("image" = "Filename"))

# Make the column names make sense 
dat_long <- dat_long %>% rename(page_type = `Page Source Type`,
                                relevant = `Relevant vs. Non Relevant`,
                                density = `Population Density`,
                                city_state = `City State`,
                                zip = `Zip Code`)

write_csv(dat_long, '/Users/mm06682/projects/school_projects/fall_2019/software_engineering/google-ad-bias-research/data/processed/final_long.csv')

#upload short dat
dat %>% write_csv("/Users/mm06682/projects/school_projects/fall_2019/software_engineering/google-ad-bias-research/data/processed/short_with_correct_exclusions_for_paige.csv")
```

Generate Summary Stats
```{r}
# Summary stats for time
dat %>% summarise(avg_time_to_complete = mean(duration)/60,
                  min_time_to_complete = min(duration)/60,
                  max_time_to_complete = max(duration)/60,
                  median_time_to_complete = median(duration)/60)

# How many men and women?
dat %>% group_by(gender) %>% summarise(n = n())

# How many of each political orientation?
dat %>% group_by(Political) %>% summarise(n = n())

# Ad feelings
dat %>% group_by(feelAboutAd) %>% summarise(n = n())

# Age distribution
dat %>% summarise(avg_age = mean(age),
                  sd_age = sd(age),
                  min_age = min(age),
                  max_age = max(age),
                  median_age = median(age))

# Race distribution
dat %>% group_by(race) %>% summarise(n = n())

# Hispanic distribution
dat %>% group_by(Hispanic) %>% summarise(n = n())

# Race, hispanic distribution
dat %>% group_by(race, Hispanic) %>% summarise(n = n())

# education distribution
dat %>% group_by(education) %>% summarise(n = n())

# employment distribution
dat %>% group_by(occupation) %>% summarise(n = n())

# education, employment distribution
dat %>% group_by(education, occupation) %>% summarise(n = n())

# political orientation
dat %>% group_by(Political) %>% summarise(n = n()) %>% write_csv("/Users/mm06682/projects/school_projects/fall_2019/software_engineering/google-ad-bias-research/data/processed/political_orientation_counts.csv")

```

Generate Some Interesting Graphs
```{r}
# Histogram grid of rating by page type and gender
dat_long %>% 
  ggplot(aes(x = rating)) + 
  geom_histogram(binwidth = 1, color = "white") +
  facet_grid(as.factor(gender) ~ as.factor(page_type), scales = "free") + #adding scales = "free" here will make "other" more visible) +
    ggtitle("Relevancy Rating Distribution by Gender and Page Type") +
  xlab("Relevancy Rating") +
  ylab("Count")

# Histogram grid of rating by gender
dat_long %>% 
  ggplot(aes(x = rating, fill = gender)) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'dodge', binwidth = 1) +
    scale_fill_manual(values=c("#69b3a2", "#404080", "#7daebd")) +
    ggtitle("Relevancy Rating Distribution by Gender") +
    guides(fill=guide_legend(title="Gender")) +
  xlab("Relevancy Rating") +
  ylab("Count")

# Histogram grid of rating by study designed relevancy and gender
dat_long %>%
  ggplot(aes(x = rating)) + 
  geom_histogram(binwidth = 1, color = "white") +
  facet_grid(as.factor(gender) ~ as.factor(relevant))

# Histogram grid of rating by specific page/ad pairing and gender
# Hard to see here, but definitely some pages in there with drastic gender differences. 
dat_long %>%
  ggplot(aes(x = rating)) + 
  geom_histogram(binwidth = 1, color = "white") +
  facet_grid(as.factor(gender) ~ as.factor(image))

# Histogram grid of rating by region
dat_long %>% 
  ggplot(aes(x = rating)) +
  geom_histogram(binwidth = 1, color = "white") +
  facet_grid(as.factor(Region) ~ as.factor(page_type)) +
  ggtitle("Relevancy Rating Distribution by Region and Page Type") +
  xlab("Relevancy Rating") +
  ylab("Count")

# Histogram grid of rating by page type and political orientation 
# Bin democrate and lean-democrat, republican and lean-republican
dat_long <- dat_long %>% 
  mutate(political_orientation = as.factor(ifelse(Political == "lean-democrat", "democrat",
                                        ifelse(Political == "lean-republican", "republican", Political)))) 
dat_long %>%
  mutate(political_orientation = fct_reorder(political_orientation, rating, .fun='length', .desc = TRUE )) %>%
  ggplot(aes(x = rating)) +
  geom_histogram(binwidth = 1, color = "white") +
  facet_grid(political_orientation ~ as.factor(page_type), scales = "free") +
  ggtitle("Relevancy Rating Distribution by Political Orientation and Page Type") +
  xlab("Relevancy Rating") +
  ylab("Count") 
  

```


Mann-Whitney Tests

So, the sig differences here are all between political and non-political pages. But, no-one seems to care about the difference between liberal and conservative pages. 

```{r}
# gender and page type to predict ratings
dat_long %>% group_by(gender, page_type, relevant, rating) %>% summarise(n = n())

# gender to predict ratings
pairwise.wilcox.test(dat_long$rating, as.factor(dat_long$gender),
            p.adjust.method = "bonferroni")

# Ignoring "other" groups, all sig p-values, across the board. 
gender_type <- pairwise.wilcox.test(dat_long$rating, interaction(dat_long$gender, dat_long$page_type), 
                     p.adjust.method = "bonferroni")

capture.output(gender_type, file = "gender_type.txt", append = FALSE)

# region and page type to predict ratings 
# Super interesting result: non-political pages and political pages have sig differences in relevancy ratings but 
region_type <- pairwise.wilcox.test(dat_long$rating, interaction(dat_long$Region, dat_long$page_type), 
                     p.adjust.method = "bonferroni")
capture.output(region_type, file = "region_type.txt", append =FALSE)

region_only <- pairwise.wilcox.test(dat_long$rating, as.factor(dat_long$Region),
                                    p.adjust.method = "bonferroni")
region_only

page_type_only <- pairwise.wilcox.test(dat_long$rating, as.factor(dat_long$page_type),
                                       p.adjust.method = "bonferroni")

page_type_only

# political orientation and page type to predict ratings
political_type <- pairwise.wilcox.test(dat_long$rating, 
                                       interaction(dat_long$Political, dat_long$page_type),
                                       p.adjust.method = "bonferroni")
capture.output(political_type, file = "political_type.txt", append = FALSE)

# What's really interesting here is that the sig p-values between dems/republicans occur only for non-political pages
# So, dems/republicans seem to agree on what belongs on political pages but they differ on what they find
# relevant to non-political pages. 

# just political orientation
pairwise.wilcox.test(dat_long$rating, dat_long$political_orientation,
                     p.adjust.method = "bonferroni")
pairwise.wilcox.test(dat_long$rating, dat_long$Political,
                     p.adjust.method = "bonferroni")
```

